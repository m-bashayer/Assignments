# -*- coding: utf-8 -*-
"""ml20bma_cw2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Y_B6WbCfogh2VWydYeda1GOz1ttl-HGm
"""

! wget https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip

! unzip Flickr8k_Dataset.zip

! git clone https://github.com/ysbecca/flickr8k-custom

"""
Parameters. You will need to customize the directories to match your
environment.

"""


##################################################################
# 
#         		DIRECTORIES
# 
################################################################### 


# path to caption data
CAPTION_DIR = "flickr8k-custom/captions/"
# path to images
IMAGE_DIR = "Flicker8k_Dataset/"

# token file names to read from
TOKEN_FILE_TRAIN = CAPTION_DIR + "Flickr8k_train.token.txt"
TOKEN_FILE_TEST = CAPTION_DIR + "Flickr8k_test.token.txt"


##################################################################
# 
#         TRAINING and VOCAB PARAMS (defaults - you may change)
# 
################################################################### 

MIN_FREQUENCY = 3

EMBED_SIZE = 256
HIDDEN_SIZE = 512
NUM_LAYERS = 1
LR = 0.001
NUM_EPOCHS = 5
LOG_STEP = 10

""" Vocabulary class """


class Vocabulary(object):
    """ Simple vocabulary wrapper which maps every unique word to an integer ID. """
    def __init__(self):
        # intially, set both the IDs and words to empty dictionaries.
        self.word2idx = {}
        self.idx2word = {}
        self.idx = 0

    def add_word(self, word):
        # if the word does not already exist in the dictionary, add it
        if not word in self.word2idx:
            self.word2idx[word] = self.idx
            self.idx2word[self.idx] = word
            # increment the ID for the next word
            self.idx += 1

    def __call__(self, word):
        # if we try to access a word in the dictionary which does not exist, return the <unk> id
        if not word in self.word2idx:
            return self.word2idx['<unk>']
        return self.word2idx[word]

    def __len__(self):
        return len(self.word2idx)

import torch
from PIL import Image
from torch.utils.data import Dataset

#from config import IMAGE_DIR



class Flickr8k_Images(Dataset):
    """ Flickr8k custom dataset to read image data only,
        compatible with torch.utils.data.DataLoader. """
    
    def __init__(self, image_ids, transform=None):
        """ Set the path for images, captions and vocabulary wrapper.
        
        Args:
            image_ids (str list): list of image ids
            transform: image transformer
        """
        self.image_ids = image_ids
        self.transform = transform


    def __getitem__(self, index):
        """ Returns image. """

        image_id = self.image_ids[index]
        path = IMAGE_DIR + str(image_id) + ".jpg"
        image = Image.open(open(path, 'rb'))

        if self.transform is not None:
            image = self.transform(image)

        return image

    def __len__(self):
        return len(self.image_ids)


class Flickr8k_Features(Dataset):
    """ Flickr8k custom dataset with features and vocab, compatible with torch.utils.data.DataLoader. """
    
    def __init__(self, image_ids, captions, vocab, features):
        """ Set the path for images, captions and vocabulary wrapper.
        
        Args:
            image_ids (str list): list of image ids
            captions (str list): list of str captions
            vocab: vocabulary wrapper
            features: torch Tensor of extracted features
        """
        self.image_ids = image_ids
        self.captions = captions
        self.vocab = vocab
        self.features = features

    def __getitem__(self, index):
        """ Returns one data pair (feature and target caption). """

        #path = IMAGE_DIR + str(self.image_ids[index]) + ".jpg"
        image_features = self.features[index]

        # convert caption (string) to word ids.
        tokens = self.captions[index].split()
        caption = []
        # build the Tensor version of the caption, with token words
        caption.append(self.vocab('<start>'))
        caption.extend([self.vocab(token) for token in tokens])
        caption.append(self.vocab('<end>'))
        target = torch.Tensor(caption)
        
        return image_features, target

    def __len__(self):
        return len(self.image_ids)

""" Encoder and decoder models """

import torch
import torch.nn as nn
import torchvision.models as models
from torch.nn.utils.rnn import pack_padded_sequence

class EncoderCNN(nn.Module):
    def __init__(self):
        """Load the pretrained ResNet-152 and replace top fc layer."""
        super(EncoderCNN, self).__init__()
        resnet = models.resnet152(pretrained=True)
         # Keep all layers except the last one
        layers = list(resnet.children())[:-1]   
        # unpack the layers and create a new Sequential
        self.resnet = nn.Sequential(*layers)
        
    def forward(self, images):
        """Extract feature vectors from input images."""
#######################################################################3333333333333333333333333333333333333333333333333333333333333333333333333333

        # QUESTION 1.2
        # TODO
      
        with torch.no_grad():
          features = self.resnet(images)
        return features


class DecoderRNN(nn.Module):
    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, max_seq_length=20):
        """Set the hyper-parameters and build the layers."""
        super(DecoderRNN, self).__init__()
        # We want a specific output size, which is the size of our embedding, so
        # we feed our extracted features from the last fc layer (dimensions 1 x 4096)

        # into a Linear layer to resize
        self.resize = nn.Linear(2048, embed_size)
        # Batch normalisation helps to speed up training
        self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)

        # What is an embedding layer?
        self.embed = nn.Embedding(vocab_size, embed_size)



        # QUESTION 1.3 DecoderRNN - define this layer 
        # TODO
        self.rnn = nn.RNN(embed_size, hidden_size, num_layers, batch_first=True)



        self.linear = nn.Linear(hidden_size, vocab_size)
        self.max_seq_length = max_seq_length
        

    def forward(self, features, captions, lengths):
        """Decode image feature vectors and generates captions."""
        embeddings = self.embed(captions)
        im_features = self.resize(features)
        im_features = self.bn(im_features)
        embeddings = torch.cat((im_features.unsqueeze(1), embeddings), 1)
        
        # What is "packing" a sequence?
        packed = pack_padded_sequence(embeddings, lengths, batch_first=True) 
        hiddens, _ = self.rnn(packed)
        outputs = self.linear(hiddens[0])
        return outputs
    

    def sample(self, features, states=None):
        """Generate captions for given image features using greedy search."""
        sampled_ids = []

        inputs = self.bn(self.resize(features)).unsqueeze(1)
        for i in range(self.max_seq_length):
            hiddens, states = self.rnn(inputs, states)          # hiddens: (batch_size, 1, hidden_size)
            outputs = self.linear(hiddens.squeeze(1))            # outputs:  (batch_size, vocab_size)
            _, predicted = outputs.max(1)                        # predicted: (batch_size)
            sampled_ids.append(predicted)
            inputs = self.embed(predicted)                       # inputs: (batch_size, embed_size)
            inputs = inputs.unsqueeze(1)                         # inputs: (batch_size, 1, embed_size)
        sampled_ids = torch.stack(sampled_ids, 1)                # sampled_ids: (batch_size, max_seq_length)
        return sampled_ids

import torch
import pandas as pd
from collections import Counter
import matplotlib.pyplot as plt

from nltk.translate.bleu_score import sentence_bleu

#from vocabulary import Vocabulary
#from config import *



def read_lines(filepath):
    """ Open the ground truth captions into memory, line by line. 
    Args:
        filepath (str): the complete path to the tokens txt file
    """
    file = open(filepath, 'r')
    lines = []

    while True: 
        # Get next line from file 
        line = file.readline() 
        if not line: 
            break
        lines.append(line.strip())
    file.close()
    return lines

def parse_lines(lines):
    """
    Parses token file captions into image_ids and captions.
    Args:
        lines (str list): str lines from token file
    Return:
        image_ids (int list): list of image ids, with duplicates
        cleaned_captions (list of lists of str): lists of words
    """
##################################################################################11111111111111111111111111111111111111111111
    image_ids = []
    spliting = [line.split('.jpg') for line in lines]
    image_ids = [iid[0] for iid in spliting]

    cleaned_captions = [caption[1].strip("\t~`!@#$%,^&*(){}[]|\/:;\";<>.?,1234567890").strip(" ") for caption in spliting]
    cleaned_captions = [word.lower() for word in cleaned_captions]
    # QUESTION 1.1
    

    return image_ids, cleaned_captions

#################################################################################22222222222222222222222222222222
def build_vocab(cleaned_captions):
    """ 
    Parses training set token file captions and builds a Vocabulary object
    Args:
        cleaned_captions (str list): cleaned list of human captions to build vocab with

    Returns:
        vocab (Vocabulary): Vocabulary object
    """
    # QUESTION 1.1
    # TODO collect words

    # create a vocab instance
    vocab = Vocabulary()
    # add the token words
    vocab.add_word('<pad>')
    vocab.add_word('<start>')
    vocab.add_word('<end>')
    vocab.add_word('<unk>')

    # TODO add the rest of the words from the cleaned captions here
    allwords = []
    allwords = [s.strip() for s in cleaned_captions]
    result = []
    for s in allwords:
        s=s.split()
        result.append(s)
    wordss = [s for S in result for s in S]
 
    for word in wordss:
      if wordss.count(word) > 3:
        vocab.add_word(word)

    return vocab

def decode_caption(sampled_ids, vocab):
  

    """ 
    Args:
        sampled_ids (int list): list of word IDs from decoder
        vocab (Vocabulary): vocab for conversion
    Return:
        predicted_caption (str): predicted string sentence
    """
    #converting the word IDs into words and stringing the caption together.

    idtoword= []
    predicted_caption = ""
    if torch.is_tensor(sampled_ids):
      sampled_ids = sampled_ids.numpy().tolist()
   # for x in sampled_ids:
    for i in sampled_ids:
      word = vocab.idx2word[i]
      idtoword.append(word)

      predicted_caption = " ".join(idtoword)

    # QUESTION 2.1


    return predicted_caption


"""
We need to overwrite the default PyTorch collate_fn() because our 
ground truth captions are sequential data of varying lengths. The default
collate_fn() does not support merging the captions with padding.

You can read more about it here:
https://pytorch.org/docs/stable/data.html#dataloader-collate-fn. 
"""
def caption_collate_fn(data):
    """ Creates mini-batch tensors from the list of tuples (image, caption).
    Args:
        data: list of tuple (image, caption). 
            - image: torch tensor of shape (3, 224, 224).
            - caption: torch tensor of shape (?); variable length.
    Returns:
        images: torch tensor of shape (batch_size, 3, 224, 224).
        targets: torch tensor of shape (batch_size, padded_length).
        lengths: list; valid length for each padded caption.
    """
    # Sort a data list by caption length from longest to shortest.
    data.sort(key=lambda x: len(x[1]), reverse=True)
    images, captions = zip(*data)

    # merge images (from tuple of 3D tensor to 4D tensor).
    # if using features, 2D tensor to 3D tensor. (batch_size, 256)
    images = torch.stack(images, 0) 

    # merge captions (from tuple of 1D tensor to 2D tensor).
    lengths = [len(cap) for cap in captions]
    targets = torch.zeros(len(captions), max(lengths)).long()
    for i, cap in enumerate(captions):
        end = lengths[i]
        targets[i, :end] = cap[:end]        
    return images, targets, lengths

"""
COMP5623M Coursework on Image Caption Generation


Forward pass through Flickr8k image data to extract and save features from
pretrained CNN.

"""


import torch
import numpy as np

import torch.nn as nn
from torchvision import transforms

#from models import EncoderCNN
#from datasets import Flickr8k_Images
#from utils import *
#from config import *



lines = read_lines(TOKEN_FILE_TRAIN)
# see what is in lines
print(lines[:2])

#########################################################################
#
#       QUESTION 1.1 Text preparation
# 
#########################################################################

image_ids, cleaned_captions = parse_lines(lines)
# to check the results after writing the cleaning function
print(image_ids[:2])
print(cleaned_captions[:2])

#vocab = build_vocab(cleaned_captions)
# to check the results
#print("Number of words in vocab:", vocab.idx)

# sample each image once
image_ids = image_ids[::5]
#print(image_ids)
#print(cleaned_captions)

# crop size matches the input dimensions expected by the pre-trained ResNet
data_transform = transforms.Compose([ 
    transforms.Resize(224), 
    transforms.CenterCrop(224), 
    transforms.ToTensor(),
    transforms.Normalize((0.485, 0.456, 0.406),   # using ImageNet norms
                         (0.229, 0.224, 0.225))])

dataset_train = Flickr8k_Images(
    image_ids=image_ids,
    transform=data_transform,
)

train_loader = torch.utils.data.DataLoader(
    dataset_train,
    batch_size=64,
    shuffle=False,
    num_workers=0,
)

# device configuration
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = EncoderCNN().to(device)



#########################################################################
#
#        QUESTION 1.2 Extracting image features
# 
#########################################################################
features = []


# TODO loop through all image data, extracting features and saving them
# no gradients needed
with torch.no_grad():
  for i, data in enumerate(train_loader, 0):              
    data = data.to(device)
    outputs = model(data)
    features.append(outputs)

features = torch.cat(features)
features=features.squeeze()
print(features.shape)

#features=features.squeeze()
print(features.shape)

torch.save(features, "features.pt")

"""
COMP5623M Coursework on Image Caption Generation


python decoder.py


"""

import torch
import numpy as np

import torch.nn as nn
from torchvision import transforms
from torch.nn.utils.rnn import pack_padded_sequence
from PIL import Image

"""
#from datasets import Flickr8k_Images, Flickr8k_Features
from models import DecoderRNN, EncoderCNN
from utils import *
from config import *
"""
# if false, train model; otherwise try loading model from checkpoint and evaluate
EVAL = False


# reconstruct the captions and vocab, just as in extract_features.py
lines = read_lines(TOKEN_FILE_TRAIN)
image_ids, cleaned_captions = parse_lines(lines)
vocab = build_vocab(cleaned_captions)


# device configuration
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

EVAL = False
# initialize the models and set the learning parameters
decoder = DecoderRNN(EMBED_SIZE, HIDDEN_SIZE, len(vocab), NUM_LAYERS).to(device)


if not EVAL:

    # load the features saved from extract_features.py
    print(len(lines))
    features = torch.load('features.pt', map_location=device)
    print("Loaded features", features.shape)

    features = features.repeat_interleave(5, 0)
    print("Duplicated features", features.shape)

    dataset_train = Flickr8k_Features(
        image_ids=image_ids,
        captions=cleaned_captions,
        vocab=vocab,
        features=features,
    )

    train_loader = torch.utils.data.DataLoader(
        dataset_train,
        batch_size=64, # change as needed
        shuffle=True,
        num_workers=0, # may need to set to 0
        collate_fn=caption_collate_fn, # explicitly overwrite the collate_fn
    )


    # loss and optimizer
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(decoder.parameters(), lr=LR)

    print(len(image_ids))
    print(len(cleaned_captions))
    print(features.shape)

#########################################################################
#
#        QUESTION 1.3 Training DecoderRNN
# 
#########################################################################

    # TODO write training loop on decoder here


    # for each batch, prepare the targets using this torch.nn.utils.rnn function
    # targets = pack_padded_sequence(captions, lengths, batch_first=True)[0]

###########################################################################################################
nepochs = 5
losses = np.zeros(nepochs)
mean_loss = [] 
epoch_nums = []
for epoch in range(nepochs):  # loop over the dataset multiple times

    running_loss = 0.0
    n = 0
    for i, data in enumerate(train_loader):
       # print(len(data))
        imfeatures, captions, lengths = data

        imfeatures = imfeatures.to(device)
        captions=captions.to(device)
     #   lengths = torch.Tensor(lengths)

        d= decoder(imfeatures ,captions ,lengths )
        targets = pack_padded_sequence(captions, lengths, batch_first=True)[0]
        # Zero the parameter gradients
        optimizer.zero_grad()
        loss = criterion(d, targets)
        loss.backward()
        optimizer.step()
    
        # accumulate loss
        running_loss += loss.item()
        n += 1
    
    losses[epoch] = meanloss = running_loss / n
    print(f"epoch: {epoch+1} loss: {meanloss : .3f}")
    mean_loss.append(meanloss)
    epoch_nums.append(epoch)

# save model after training
decoder_ckpt = torch.save(decoder, "decoder.ckpt")

EVAL = True
# if we already trained, and EVAL == True, reload saved model
if EVAL == True:

    data_transform = transforms.Compose([ 
        transforms.Resize(224),     
        transforms.CenterCrop(224), 
        transforms.ToTensor(),
        transforms.Normalize((0.485, 0.456, 0.406),   # using ImageNet norms
                             (0.229, 0.224, 0.225))])


    test_lines = read_lines(TOKEN_FILE_TEST)
    test_image_ids, test_cleaned_captions = parse_lines(test_lines)
    
    test_image_ids_unique = test_image_ids[::5]
    
    dataset_test = Flickr8k_Images(
        image_ids=test_image_ids_unique,
        transform = data_transform
    )

  
    test_loader = torch.utils.data.DataLoader(
    dataset_test,
    batch_size=64,
    shuffle=False,
    num_workers=0,
)
    # load models
    encoder = EncoderCNN().to(device)
    decoder = torch.load("decoder.ckpt").to(device)
    encoder.eval()
    decoder.eval() # generate caption, eval mode to not influence batchnorm

#########################################################################
#
#        QUESTION 2.1 Generating predictions on test data
# 
#########################################################################


    # TODO define decode_caption() function in utils.py
    # predicted_caption = decode_caption(word_ids, vocab)
image_ids = test_image_ids[::5]
#image_ids = image_ids[::5]

data_transform = transforms.Compose([ 
    transforms.Resize(224), 
    transforms.CenterCrop(224), 
    transforms.ToTensor(),
    transforms.Normalize((0.485, 0.456, 0.406),   # using ImageNet norms
                         (0.229, 0.224, 0.225))])

Testimage = Flickr8k_Images(
    image_ids=image_ids,
    transform=data_transform,
)

train_loader = torch.utils.data.DataLoader(
    Testimage,
    batch_size=64,
    shuffle=False,
    num_workers=0,
)
# device configuration
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = EncoderCNN().to(device)

testfeatures = []
with torch.no_grad():
  for i, data in enumerate(train_loader, 0):              
    data = data.to(device)
    testoutputs = model(data)
    #testoutputs = decoder.sample(testfeatures)
    testfeatures.append(testoutputs)

testfeatures = torch.cat(testfeatures)
#testfeatures = testfeatures.squeeze()
print(testfeatures.shape)

testfeatures = testfeatures.squeeze()

word_ids=[]
with torch.no_grad():
  for i in test_loader:
    outputs = encoder(i)
    features = outputs.squeeze()
    dec_sample = decoder.sample(features)
    for j in dec_sample:
      word_ids.append(j)

from nltk.translate.bleu_score import sentence_bleu
#2.1 + 2.2.2
count = 0
for i, data in enumerate(word_ids[:300]):
  refsent = []
  s = decode_caption(word_ids[i], vocab)
  tt = s.split("<end>", 1)
  t1=tt[0]
  t1 = t1.replace("<start>","")
#tt = tt.replace("<end>","")
  t1 = t1.strip()
  print("The five reference captions are:")
  for j in range(5):
    re = test_cleaned_captions[count]
    refsent.append(re.split())
    count+=1
    print(re)
  print("The generated caption is:")
  print(t1)
  reference = refsent
  candidate = t1.split()
  score = sentence_bleu(reference, candidate)
  
  print("The BLEU score is: ", score )
  print("The image id is: ",image_ids[i])
  print('\n')

#2.2.1
total = 0
count = 0
H_B = 0.5
H_B_i = 0
L_B = 0.5
L_B_i = 0
for i, data in enumerate(word_ids):
  refsent = []
  s = decode_caption(word_ids[i], vocab)
  tt = s.split("<end>", 1)
  t1=tt[0]
  t1 = t1.replace("<start>","")
  t1 = t1.strip()

  for j in range(5):
    refsent.append(test_cleaned_captions[count].split())
    count+=1
  reference = refsent
  candidate = t1.split()
  score = sentence_bleu(reference, candidate)
  total = total + score

  if score > H_B:
    H_B = score
    H_B_i= image_ids[i]
  if score < L_B:
    L_B = score
    L_B_i=image_ids[i]

mean_score = total / len(word_ids) 
print("The mean is: ", mean_score )
print("The higher score is: ",H_B, " in image: ",H_B_i)
print("The lower score is: ",L_B, " in image: ", L_B_i)

#########################################################################
#
#        QUESTION 2.2-3 Caption evaluation via text similarity 
# 
#########################################################################


    # Feel free to add helper functions to utils.py as needed,
    # documenting what they do in the code and in your report

from sklearn.metrics.pairwise import cosine_similarity

def sentence_to_wordids(sentence):
  sentence = sentence.split()
  words_to_ids = []
  for word in sentence:
    words_to_ids.append(vocab(word))
  return words_to_ids

######
count = 0
for i, data in enumerate(word_ids[:300]):

   emdbed_ref_avg_5 = np.zeros(EMBED_SIZE)

   s = decode_caption(word_ids[i], vocab)
   tt = s.split("<end>", 1)
   tt=tt[0]
   tt = tt.replace("<start>","")
   tt = tt.strip()
   print(tt)
   captions_to_vec = sentence_to_wordids(tt)
  
   calist=[]
   for wordid in captions_to_vec:
     caption_to_tensor = torch.tensor(wordid)
     embded_caption = decoder.embed(caption_to_tensor)
     calist.append(embded_caption)
    
   calist=torch.stack(calist)
   emdbed_caption_avg= torch.mean(calist, dim=0)
   emdbed_caption_avg=emdbed_caption_avg.cpu().detach().clone().numpy()

   print("The five reference captions are:")
   refsent2 = []
   ind =[]
   for j in range(5):
     txt = test_cleaned_captions[count]
     refsent2.append(txt)
     print(txt)
     count+=1
   for x in refsent2:
     ind.append(sentence_to_wordids(x))

   print("The image id is: ",image_ids[i])
  # print('\n')
   scores = 0
   
   for x in ind:
     relist=[]
     for y in x:
       word_to_vec= torch.tensor(y)
       embded_word = decoder.embed(word_to_vec)
       relist.append(embded_word)

     relist=torch.stack(relist)
     emdbed_ref_avg = torch.mean(relist, dim=0)
     emdbed_ref_avg=emdbed_ref_avg.cpu().detach().clone().numpy()

     emdbed_ref_avg=emdbed_ref_avg.reshape(1, 256)
     emdbed_caption_avg=emdbed_caption_avg.reshape(1, 256)

     C_S = cosine_similarity(emdbed_ref_avg, emdbed_caption_avg)
     C_S = float(C_S)
     print(C_S)
     scores +=C_S
    
   
   final_score = scores/5

   print("the avg score is",final_score)
   print('\n')

count = 0
final_of_finals = 0
for i, data in enumerate(word_ids):

   emdbed_ref_avg_5 = np.zeros(EMBED_SIZE)

   s = decode_caption(word_ids[i], vocab)
   tt = s.split("<end>", 1)
   tt=tt[0]
   tt = tt.replace("<start>","")
   tt = tt.strip()
   captions_to_vec = sentence_to_wordids(tt)

   calist=[]
   for wordid in captions_to_vec:
     caption_to_tensor = torch.tensor(wordid)
     embded_caption = decoder.embed(caption_to_tensor)

     calist.append(embded_caption)

   calist=torch.stack(calist)
   emdbed_caption_avg= torch.mean(calist, dim=0)
   emdbed_caption_avg=emdbed_caption_avg.cpu().detach().clone().numpy()

   refsent2 = []
   ind =[]
   for j in range(5):
     txt = test_cleaned_captions[count]
     refsent2.append(txt)
     count+=1

   for x in refsent2:
     ind.append(sentence_to_wordids(x))

   scores = 0
   
   for x in ind:
     relist=[]
     for y in x:
       word_to_vec= torch.tensor(y)
       embded_word = decoder.embed(word_to_vec)

       relist.append(embded_word)

     relist=torch.stack(relist)
     emdbed_ref_avg = torch.mean(relist, dim=0)

     emdbed_ref_avg=emdbed_ref_avg.cpu().detach().clone().numpy()

     emdbed_ref_avg=emdbed_ref_avg.reshape(1, 256)
     emdbed_caption_avg=emdbed_caption_avg.reshape(1, 256)

     C_S = cosine_similarity(emdbed_ref_avg, emdbed_caption_avg)
     C_S = float(C_S)

     
     scores +=C_S
    
   final_score = scores/5
   
   final_of_finals+= final_score

final_of_finals=final_of_finals/len(word_ids)

print("The mean is: ",final_of_finals)
print("The mean in the range of [0.0 - 1.0] is: ", (final_of_finals+1)/2 )

'''#2.3
count = 0
final_of_finals = 0

for i, data in enumerate(word_ids):

   emdbed_ref_avg_5 = np.zeros(EMBED_SIZE)

   s = decode_caption(word_ids[i], vocab)
   tt = s.split("<end>", 1)
   tt=tt[0]
   tt = tt.replace("<start>","")
   tt = tt.strip()
   captions_to_vec = sentence_to_wordids(tt)

   calist=[]
   for wordid in captions_to_vec:
     caption_to_tensor = torch.tensor(wordid)
     embded_caption = decoder.embed(caption_to_tensor)

     calist.append(embded_caption)

   calist=torch.stack(calist)
   emdbed_caption_avg= torch.mean(calist, dim=0)
   emdbed_caption_avg=emdbed_caption_avg.cpu().detach().clone().numpy()

   refsent2 = []
   ind =[]
   for j in range(5):
     txt = test_cleaned_captions[count]
     refsent2.append(txt)
     count+=1

   for x in refsent2:
     ind.append(sentence_to_wordids(x))

   scores = 0
   
   for x in ind:
     relist=[]
     for y in x:
       word_to_vec= torch.tensor(y)
       embded_word = decoder.embed(word_to_vec)

       relist.append(embded_word)

     relist=torch.stack(relist)
     emdbed_ref_avg = torch.mean(relist, dim=0)

     emdbed_ref_avg=emdbed_ref_avg.cpu().detach().clone().numpy()

     emdbed_ref_avg=emdbed_ref_avg.reshape(1, 256)
     emdbed_caption_avg=emdbed_caption_avg.reshape(1, 256)

     C_S = cosine_similarity(emdbed_ref_avg, emdbed_caption_avg)
     C_S+=1
     C_S = C_S/2
     scores +=C_S

   final_score = scores/5
   final_of_finals+=final_score
print("The mean is: ",float(final_of_finals/len(word_ids)))'''

#2.3.2 Show one example where both methods give similar scores
count = 0
final_of_finals = 0
for i, data in enumerate(word_ids):

   s = decode_caption(word_ids[i], vocab)
   tt = s.split("<end>", 1)
   tt=tt[0]
   tt = tt.replace("<start>","")
   tt = tt.strip()
   captions_to_vec = sentence_to_wordids(tt)

   calist=[]
   for wordid in captions_to_vec:
     caption_to_tensor = torch.tensor(wordid)
     embded_caption = decoder.embed(caption_to_tensor)
     calist.append(embded_caption)

   calist=torch.stack(calist)
   emdbed_caption_avg= torch.mean(calist, dim=0)
   emdbed_caption_avg=emdbed_caption_avg.cpu().detach().clone().numpy()
   
   refsent = []
   refsent2 = []
   ind =[]
   for j in range(5):
     txt = test_cleaned_captions[count]
     refsent2.append(txt)
     refsent.append(txt.split())
     count+=1
   for x in refsent2:
     ind.append(sentence_to_wordids(x))

##
   reference = refsent
   candidate = tt.split()
   Bleu_s = sentence_bleu(reference, candidate)
##
   scores = 0
   for x in ind:
     relist=[]
     for y in x:
       word_to_vec= torch.tensor(y)
       embded_word = decoder.embed(word_to_vec)
       relist.append(embded_word)

     relist=torch.stack(relist)
     emdbed_ref_avg = torch.mean(relist, dim=0)
     emdbed_ref_avg=emdbed_ref_avg.cpu().detach().clone().numpy()
     emdbed_ref_avg=emdbed_ref_avg.reshape(1, 256)
     emdbed_caption_avg=emdbed_caption_avg.reshape(1, 256)

     C_S = cosine_similarity(emdbed_ref_avg, emdbed_caption_avg)
     C_S = float(C_S)
     C_S+=1
     C_S = C_S/2
     scores +=C_S
   final_score = scores/5
   
   final_score = "{:.2f}".format(final_score)
   Bleu_s = "{:.2f}".format(Bleu_s)

   if final_score == Bleu_s:
     print('\n')
     print("##################################################################################################################################################################")
     print('\n')
     print("The cosine similarity score =", final_score, " and the bleu score = ",Bleu_s)
     print("The image id is: ",image_ids[i])
     print("The generated caption is:")
     print(tt)
     print("The five reference captions are:")
     for r in refsent2:
       print(r)
     print('\n')



  # final_of_finals+=final_score
#print("The mean is: ",float(final_of_finals/len(word_ids)))

image_ids[1]